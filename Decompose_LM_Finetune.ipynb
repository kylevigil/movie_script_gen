{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                                  BertConfig, BertForMaskedLM, BertTokenizer,\n",
    "                                  GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
    "                                  OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
    "                                  RobertaConfig, RobertaForMaskedLM, RobertaTokenizer,\n",
    "                                  DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, args, file_path='train', block_size=512):\n",
    "        assert os.path.isfile(file_path)\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(directory, args['model_name_or_path'] + '_cached_lm_' + str(block_size) + '_' + filename)\n",
    "\n",
    "        if os.path.exists(cached_features_file):\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, 'rb') as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            \n",
    "            df = pd.read_pickle(\"../data/train.pkl\")\n",
    "            \n",
    "            genre_tok = dict()\n",
    "            for genre in df.genres.unique():\n",
    "                g = genre.replace(\",\", \" \").replace(\"Sci-Fi\", \"Science Fiction\") + \"~\"\n",
    "                genre_tok[genre] = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(g))\n",
    "\n",
    "            for script, genre in zip(df.script, df.genres):\n",
    "                tokenized_genre = genre_tok[genre]\n",
    "                tokenized_script = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(script))\n",
    "\n",
    "                temp_block_size = block_size - len(tokenized_genre)\n",
    "\n",
    "                for i in range(0, len(tokenized_script)-temp_block_size+1, temp_block_size):\n",
    "                    self.examples.append(tokenized_genre+tokenized_script[i:i+temp_block_size])\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, 'wb') as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
    "    dataset = TextDataset(tokenizer, args, file_path=args['eval_data_file'] if evaluate else args['train_data_file'], block_size=args['block_size'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    if args['n_gpu'] > 0:\n",
    "        torch.cuda.manual_seed_all(args['seed'])\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix, use_mtime=False):\n",
    "    if not args['save_total_limit']:\n",
    "        return\n",
    "    if args['save_total_limit'] <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    glob_checkpoints = glob.glob(os.path.join(args['output_dir'], '{}-*'.format(checkpoint_prefix)))\n",
    "    if len(glob_checkpoints) <= args['save_total_limit']:\n",
    "        return\n",
    "\n",
    "    ordering_and_checkpoint_path = []\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match('.*{}-([0-9]+)'.format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args['save_total_limit'])\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)\n",
    "\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, args):\n",
    "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    probability_matrix = torch.full(labels.shape, args['mlm_probability'])\n",
    "    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args['local_rank'] in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args['train_batch_size'] = args['per_gpu_train_batch_size'] * max(1, args['n_gpu'])\n",
    "    train_sampler = RandomSampler(train_dataset) if args['local_rank'] == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "\n",
    "    if args['max_steps'] > 0:\n",
    "        t_total = args['max_steps']\n",
    "        args['num_train_epochs'] = args['max_steps'] // (len(train_dataloader) // args['gradient_accumulation_steps']) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total)\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args['n_gpu'] > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args['local_rank'] != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args['local_rank']],\n",
    "                                                          output_device=args['local_rank'],\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args['per_gpu_train_batch_size'])\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                   args['train_batch_size'] * args['gradient_accumulation_steps'] * (torch.distributed.get_world_size() if args['local_rank'] != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\", disable=args['local_rank'] not in [-1, 0])\n",
    "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args['local_rank'] not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = mask_tokens(batch, tokenizer, args) if args['mlm'] else (batch, batch)\n",
    "            inputs = inputs.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            model.train()\n",
    "            outputs = model(inputs, masked_lm_labels=labels) if args['mlm'] else model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args['n_gpu'] > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                if args['fp16']:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args['local_rank'] in [-1, 0] and args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    if args['local_rank'] == -1 and args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['local_rank'] in [-1, 0] and args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                    checkpoint_prefix = 'checkpoint'\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args['output_dir'], '{}-{}'.format(checkpoint_prefix, global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "            if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args['local_rank'] in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "args = {\n",
    "    \"train_data_file\": \"../data/train.pkl\",\n",
    "    \"output_dir\": \"tempoutput\",\n",
    "    \"eval_data_file\": \"../data/test.pkl\",\n",
    "    \"model_type\": \"gpt2\",\n",
    "    \"model_name_or_path\": \"gpt2\",\n",
    "\n",
    "    \"mlm\":False,\n",
    "    \"mlm_probability\":0.15,\n",
    "    \"config_name\":\"\",\n",
    "    \"tokenizer_name\":\"\",\n",
    "    \"cache_dir\":\"\",\n",
    "    \"block_size\":-1,\n",
    "    \"do_train\":True,\n",
    "    \"do_eval\":False,\n",
    "    \"evaluate_during_training\":False,\n",
    "    \"do_lower_case\":False,\n",
    "    \n",
    "    \"per_gpu_train_batch_size\":1,\n",
    "    \"per_gpu_eval_batch_size\":4,\n",
    "    \"gradient_accumulation_steps\":1,\n",
    "    \"learning_rate\":5e-5,\n",
    "    \"weight_decay\":0.0,\n",
    "    \"adam_epsilon\":1e-8,\n",
    "    \"max_grad_norm\":1,\n",
    "    \"num_train_epochs\":1.0,\n",
    "    \"max_steps\":-1,\n",
    "    \"warmup_steps\":-1,\n",
    "    \n",
    "    \"logging_steps\":50,\n",
    "    \"save_steps\":50,\n",
    "    \"save_total_limit\":None,\n",
    "    \"eval_all_checkpoints\":True,\n",
    "    \"no_cuda\":False,\n",
    "    \"overwrite_output_dir\":True,\n",
    "    \"overwrite_cache\":False,\n",
    "    \"seed\":42,\n",
    "    \n",
    "    \"fp16\":True,\n",
    "    \"fp16_opt_level\":\"O3\",\n",
    "    \"local_rank\":-1,\n",
    "    \"server_ip\":\"\",\n",
    "    \"server_port\":\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2019 21:47:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n"
     ]
    }
   ],
   "source": [
    "if args['model_type'] in [\"bert\", \"roberta\", \"distilbert\"] and not args['mlm']:\n",
    "    raise ValueError(\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
    "                     \"flag (masked language modeling).\")\n",
    "if args['eval_data_file'] is None and args['do_eval']:\n",
    "    raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "                     \"or remove the --do_eval argument.\")\n",
    "\n",
    "if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args['server_ip'] and args['server_port']:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args['server_ip'], args['server_port']), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args['local_rank'] == -1 or args['no_cuda']:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args['no_cuda'] else \"cpu\")\n",
    "    args['n_gpu'] = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "    device = torch.device(\"cuda\", args['local_rank'])\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args['n_gpu'] = 1\n",
    "args['device'] = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO if args['local_rank'] in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "                args['local_rank'], device, args['n_gpu'], bool(args['local_rank'] != -1), args['fp16'])\n",
    "\n",
    "# Set seed\n",
    "set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2019 21:47:37 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /tmp/xdg-cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "12/03/2019 21:47:37 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "12/03/2019 21:47:37 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /tmp/xdg-cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "12/03/2019 21:47:37 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /tmp/xdg-cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "12/03/2019 21:47:38 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /tmp/xdg-cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "if args['local_rank'] not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "config = config_class.from_pretrained(args['config_name'] if args['config_name'] else args['model_name_or_path'],\n",
    "                                      cache_dir=args['cache_dir'] if args['cache_dir'] else None)\n",
    "tokenizer = tokenizer_class.from_pretrained(args['tokenizer_name'] if args['tokenizer_name'] else args['model_name_or_path'],\n",
    "                                            do_lower_case=args['do_lower_case'],\n",
    "                                            cache_dir=args['cache_dir'] if args['cache_dir'] else None)\n",
    "if args['block_size'] <= 0:\n",
    "    args['block_size'] = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "args['block_size'] = min(args['block_size'], tokenizer.max_len_single_sentence)\n",
    "model = model_class.from_pretrained(args['model_name_or_path'],\n",
    "                                    from_tf=bool('.ckpt' in args['model_name_or_path']),\n",
    "                                    config=config,\n",
    "                                    cache_dir=args['cache_dir'] if args['cache_dir'] else None)\n",
    "model.to(args['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2019 21:47:47 - INFO - __main__ -   Training/evaluation parameters {'train_data_file': '../data/train.pkl', 'output_dir': 'tempoutput', 'eval_data_file': '../data/test.pkl', 'model_type': 'gpt2', 'model_name_or_path': 'gpt2', 'mlm': False, 'mlm_probability': 0.15, 'config_name': '', 'tokenizer_name': '', 'cache_dir': '', 'block_size': 1024, 'do_train': True, 'do_eval': False, 'evaluate_during_training': False, 'do_lower_case': False, 'per_gpu_train_batch_size': 1, 'per_gpu_eval_batch_size': 4, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1, 'num_train_epochs': 1.0, 'max_steps': -1, 'warmup_steps': -1, 'logging_steps': 50, 'save_steps': 50, 'save_total_limit': None, 'eval_all_checkpoints': True, 'no_cuda': False, 'overwrite_output_dir': True, 'overwrite_cache': False, 'seed': 42, 'fp16': True, 'fp16_opt_level': 'O3', 'local_rank': -1, 'server_ip': '', 'server_port': '', 'n_gpu': 1, 'device': device(type='cuda')}\n",
      "12/03/2019 21:47:47 - INFO - __main__ -   Creating features from dataset file at ../data\n",
      "12/03/2019 21:51:50 - INFO - __main__ -   Saving features into cached file ../data/gpt2_cached_lm_1024_train.pkl\n"
     ]
    }
   ],
   "source": [
    "if args['local_rank'] == 0:\n",
    "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "# Training\n",
    "if args['do_train']:\n",
    "    if args['local_rank'] not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2019 21:38:06 - INFO - __main__ -   ***** Running training *****\n",
      "12/03/2019 21:38:06 - INFO - __main__ -     Num examples = 116642\n",
      "12/03/2019 21:38:06 - INFO - __main__ -     Num Epochs = 1\n",
      "12/03/2019 21:38:06 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
      "12/03/2019 21:38:06 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "12/03/2019 21:38:06 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "12/03/2019 21:38:06 - INFO - __main__ -     Total optimization steps = 116642\n",
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/116642 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O3:  Pure FP16 training.\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O3\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : False\n",
      "master_weights         : False\n",
      "loss_scale             : 1.0\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration:   0%|          | 1/116642 [00:00<21:38:51,  1.50it/s]\u001b[A\n",
      "Iteration:   0%|          | 2/116642 [00:00<17:14:54,  1.88it/s]\u001b[A\n",
      "Iteration:   0%|          | 3/116642 [00:01<13:52:28,  2.34it/s]\u001b[A\n",
      "Iteration:   0%|          | 4/116642 [00:01<11:31:25,  2.81it/s]\u001b[A\n",
      "Iteration:   0%|          | 5/116642 [00:01<9:55:21,  3.27it/s] \u001b[A\n",
      "Iteration:   0%|          | 6/116642 [00:01<8:43:07,  3.72it/s]\u001b[A\n",
      "Iteration:   0%|          | 7/116642 [00:01<8:00:46,  4.04it/s]\u001b[A\n",
      "Iteration:   0%|          | 8/116642 [00:02<7:27:38,  4.34it/s]\u001b[A\n",
      "Iteration:   0%|          | 9/116642 [00:02<7:01:55,  4.61it/s]\u001b[A\n",
      "Iteration:   0%|          | 10/116642 [00:02<6:48:14,  4.76it/s]\u001b[A\n",
      "Iteration:   0%|          | 11/116642 [00:02<6:40:15,  4.86it/s]\u001b[A\n",
      "Iteration:   0%|          | 12/116642 [00:02<6:31:31,  4.96it/s]\u001b[A\n",
      "Iteration:   0%|          | 13/116642 [00:02<6:27:16,  5.02it/s]\u001b[A\n",
      "Iteration:   0%|          | 14/116642 [00:03<6:25:28,  5.04it/s]\u001b[A\n",
      "Iteration:   0%|          | 15/116642 [00:03<6:25:18,  5.04it/s]\u001b[A\n",
      "Iteration:   0%|          | 16/116642 [00:03<6:23:16,  5.07it/s]\u001b[A\n",
      "Iteration:   0%|          | 17/116642 [00:03<6:23:36,  5.07it/s]\u001b[A\n",
      "Iteration:   0%|          | 18/116642 [00:03<6:23:08,  5.07it/s]\u001b[A\n",
      "Iteration:   0%|          | 19/116642 [00:04<6:23:04,  5.07it/s]\u001b[A\n",
      "Iteration:   0%|          | 20/116642 [00:04<6:23:17,  5.07it/s]\u001b[A\n",
      "Iteration:   0%|          | 21/116642 [00:04<6:23:46,  5.06it/s]\u001b[A\n",
      "Iteration:   0%|          | 22/116642 [00:04<6:22:23,  5.08it/s]\u001b[A\n",
      "Iteration:   0%|          | 23/116642 [00:04<6:24:25,  5.06it/s]\u001b[A\n",
      "Iteration:   0%|          | 24/116642 [00:05<6:20:47,  5.10it/s]\u001b[A\n",
      "Iteration:   0%|          | 25/116642 [00:05<6:19:41,  5.12it/s]\u001b[A\n",
      "Iteration:   0%|          | 26/116642 [00:05<6:18:20,  5.14it/s]\u001b[A\n",
      "Iteration:   0%|          | 27/116642 [00:05<6:20:35,  5.11it/s]\u001b[A\n",
      "Iteration:   0%|          | 28/116642 [00:05<6:20:34,  5.11it/s]\u001b[A\n",
      "Iteration:   0%|          | 29/116642 [00:06<6:23:00,  5.07it/s]\u001b[A\n",
      "Iteration:   0%|          | 30/116642 [00:06<6:21:11,  5.10it/s]\u001b[A\n",
      "Iteration:   0%|          | 31/116642 [00:06<6:20:07,  5.11it/s]\u001b[A\n",
      "Iteration:   0%|          | 32/116642 [00:06<6:22:42,  5.08it/s]\u001b[A\n",
      "Iteration:   0%|          | 33/116642 [00:06<6:17:01,  5.15it/s]\u001b[A\n",
      "Iteration:   0%|          | 34/116642 [00:07<6:17:17,  5.15it/s]\u001b[A\n",
      "Iteration:   0%|          | 35/116642 [00:07<6:16:41,  5.16it/s]\u001b[A\n",
      "Iteration:   0%|          | 36/116642 [00:07<6:09:59,  5.25it/s]\u001b[A\n",
      "Iteration:   0%|          | 37/116642 [00:07<6:05:40,  5.31it/s]\u001b[A\n",
      "Iteration:   0%|          | 38/116642 [00:07<6:08:56,  5.27it/s]\u001b[A\n",
      "Iteration:   0%|          | 39/116642 [00:08<6:12:01,  5.22it/s]\u001b[A\n",
      "Iteration:   0%|          | 40/116642 [00:08<6:07:31,  5.29it/s]\u001b[A\n",
      "Iteration:   0%|          | 41/116642 [00:08<6:07:35,  5.29it/s]\u001b[A\n",
      "Iteration:   0%|          | 42/116642 [00:08<6:12:37,  5.22it/s]\u001b[A\n",
      "Iteration:   0%|          | 43/116642 [00:08<6:12:02,  5.22it/s]\u001b[A\n",
      "Iteration:   0%|          | 44/116642 [00:08<6:15:21,  5.18it/s]\u001b[A\n",
      "Iteration:   0%|          | 45/116642 [00:09<6:16:26,  5.16it/s]\u001b[A\n",
      "Iteration:   0%|          | 46/116642 [00:09<6:16:57,  5.16it/s]\u001b[A\n",
      "Iteration:   0%|          | 47/116642 [00:09<6:17:59,  5.14it/s]\u001b[A\n",
      "Iteration:   0%|          | 48/116642 [00:09<6:17:26,  5.15it/s]\u001b[A\n",
      "Iteration:   0%|          | 49/116642 [00:09<6:17:53,  5.14it/s]\u001b[A12/03/2019 21:38:16 - INFO - transformers.configuration_utils -   Configuration saved in tempoutput/checkpoint-50/config.json\n",
      "12/03/2019 21:38:18 - INFO - transformers.modeling_utils -   Model weights saved in tempoutput/checkpoint-50/pytorch_model.bin\n",
      "12/03/2019 21:38:18 - INFO - __main__ -   Saving model checkpoint to tempoutput/checkpoint-50\n",
      "\n",
      "Iteration:   0%|          | 50/116642 [00:11<22:08:06,  1.46it/s]\u001b[A\n",
      "Iteration:   0%|          | 51/116642 [00:11<17:23:32,  1.86it/s]\u001b[A\n",
      "Iteration:   0%|          | 52/116642 [00:12<14:04:14,  2.30it/s]\u001b[A\n",
      "Iteration:   0%|          | 53/116642 [00:12<11:40:07,  2.78it/s]\u001b[A\n",
      "Iteration:   0%|          | 54/116642 [00:12<10:01:15,  3.23it/s]\u001b[A\n",
      "Iteration:   0%|          | 55/116642 [00:12<8:55:27,  3.63it/s] \u001b[A\n",
      "Iteration:   0%|          | 56/116642 [00:12<8:08:24,  3.98it/s]\u001b[A\n",
      "Iteration:   0%|          | 57/116642 [00:13<7:27:55,  4.34it/s]\u001b[A\n",
      "Iteration:   0%|          | 58/116642 [00:13<7:05:55,  4.56it/s]\u001b[A\n",
      "Iteration:   0%|          | 59/116642 [00:13<6:44:15,  4.81it/s]\u001b[A\n",
      "Iteration:   0%|          | 60/116642 [00:13<6:28:24,  5.00it/s]\u001b[A\n",
      "Iteration:   0%|          | 61/116642 [00:13<6:32:31,  4.95it/s]\u001b[A\n",
      "Iteration:   0%|          | 62/116642 [00:14<6:31:48,  4.96it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6d87e0cb8675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-51f15fbd9e09>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fp16'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args['local_rank'] == 0:\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "if args['do_train'] and (args['local_rank'] == -1 or torch.distributed.get_rank() == 0):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(args['output_dir']) and args['local_rank'] in [-1, 0]:\n",
    "        os.makedirs(args['output_dir'])\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args['output_dir'])\n",
    "    tokenizer.save_pretrained(args['output_dir'])\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = model_class.from_pretrained(args['output_dir'])\n",
    "    tokenizer = tokenizer_class.from_pretrained(args['output_dir'], do_lower_case=args['do_lower_case'])\n",
    "    model.to(args['device'])\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if args['do_eval'] and args['local_rank'] in [-1, 0]:\n",
    "    checkpoints = [args['output_dir']]\n",
    "    if args['eval_all_checkpoints']:\n",
    "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args['output_dir'] + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "        logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "        prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "\n",
    "        model = model_class.from_pretrained(checkpoint)\n",
    "        model.to(args['device'])\n",
    "        result = evaluate(args, model, tokenizer, prefix=prefix)\n",
    "        result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "        results.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
